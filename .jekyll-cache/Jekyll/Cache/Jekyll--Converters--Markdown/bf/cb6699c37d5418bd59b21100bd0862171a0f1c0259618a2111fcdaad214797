I"~<p><em>Click title link for more info about each project!</em></p>
<details>
  <summary><h2 class="project-title">Socialyze: Friendship Discovery</h2></summary>

  <p>Socialyze is a web-based social media platform for MIT students to connect with each other, while providing a forum for students to discover events and other group activities on campus.</p>

  <ul>
    <li>Curated seamless user experience using functional CSS animations and the Chakra UI component library (React.js).</li>
    <li>Designed robust backend in Node.js (MongoDB and Next.js), using GraphQL APIs for relevant database access. Hosted unsupervised ML-based recommendation system on this backend to find ideal matches for our users based on previous activity.</li>
    <li>Built real-time messaging system from scratch with individual and group message capabilities (Express.js, Socket.io).</li>
  </ul>

</details>

<iframe width="750" height="420" src="https://www.youtube.com/embed/VRR51J8598w" title="Socialyze: Discover Your Interests" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<hr />

<details>
  <summary><h2 class="project-title">Tango: AI Dance Coach</h2></summary>

  <p>Tango is a computer-vision powered dance coaching software that provides users with live feedback and summary metrics for any dance that they would like to learn. Users are able to upload a video of our choosing, which they can then follow along and learn with our live annotations! </p>

  <ul>
    <li>Implemented MediaPipe's Pose Estimation algorithm to track users' limbs using their laptop webcam. </li>
    <li>Processed uploaded video using MediaPipe and drew live annotations on these videos using OpenCV, tracking the positions of each of the performers' limbs and superimposing them onto our user's body.</li>
    <li>Used dynamic time warping over a sliding window to find the similarity between limb angles over time, estimating accuracy scores (Python, FastAPI)</li>
    <li>Developed UI to guide users through the process of uploading, dancing, and learning their results (React.js, Chakra UI)</li>
  </ul>

</details>

<iframe width="750" height="420" src="https://www.youtube.com/embed/8-6WGpPql-4" title="Tango Demo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<hr />
<details>
  <summary><h2 class="project-title">AI-Powered Gesture Interface For Automobile Control</h2></summary>

  <p>Noticing the rapid innovations being worked on in the field of human-computer interfaces for personal vehicles, as well as the need for an accessible and solution, our team of 5 developed an interconnected embedded system to prototype a car interface controlled primarily through the use of computer vision and hand gestures.</p>

  <ul>
    <li>Utilized the ESP32 microcontroller’s HC-06 Bluetooth and WiFi capabilities (coded using C++ and the Arduino framework)</li>
    <li>Developed a backend computer vision algorithm to detect hand gestures from still Arducam images (developed with OpenCV in Python)</li>
    <li>Integrated several pieces of crucial hardware such as the Arducam and DFPlayer MP3 system to provide a fully integrated user experience that interfaces with the user's mobile device.</li>
    <li>Demonstrated prototype for a gesture-based control system for a car's navigation, media control, temperature, and communication capabilities.</li>
  </ul>

</details>

<iframe width="750" height="420" src="https://www.youtube.com/embed/265pv4jGRmo" title="AI-Powered Gesture Interface For Automobiles" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<hr />
<details>
  <summary><h2 class="project-title">Mission Connected</h2></summary>

  <p>Noticing the rapid innovations being worked on in the field of human-computer interfaces for personal vehicles, as well as the need for an accessible and solution, our team of 5 developed an interconnected embedded system to prototype a car interface controlled primarily through the use of computer vision and hand gestures.</p>

  <ul>
    <li>Utilized the ESP32 microcontroller’s HC-06 Bluetooth and WiFi capabilities (coded using C++ and the Arduino framework)</li>
    <li>Developed a backend computer vision algorithm to detect hand gestures from still Arducam images (developed with OpenCV in Python)</li>
    <li>Integrated several pieces of crucial hardware such as the Arducam and DFPlayer MP3 system to provide a fully integrated user experience that interfaces with the user's mobile device.</li>
    <li>Demonstrated prototype for a gesture-based control system for a car's navigation, media control, temperature, and communication capabilities.</li>
  </ul>

</details>

<p><a href="https://github.com/anirudhv27/MissionConnectediOS">Github Repo</a></p>

<iframe width="750" height="420" src="https://www.youtube.com/embed/RuD-W7nY72k" title="Mission Connected Demo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

:ET